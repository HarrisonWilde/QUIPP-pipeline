#!/usr/bin/env bash
set -euo pipefail

SGFINIT=${SGFROOT:+$SGFROOT/}sgfinit
SGFGEN=${SGFROOT:+$SGFROOT/}sgfgen
SGFEXT=${SGFROOT:+$SGFROOT/}sgfextract


echo $SGFINIT

PREPARE_DATA_PY=$(dirname $0)/prepare_data.py

# this is run from
# $QUIPP_ROOT/synth-output/$run-input-name/
#
# it is run like:
#
#   run $parameter_json $data_path_prefix $output_dir
#
# where
#   parameter_json: filename of the parameter json
#   data_path_prefix: path and filename prefix of the data (append .csv, .json)
#   output_dir: directory

parameter_json="$1"
data_path_prefix="$2"
output_dir="$3"

mkdir -p $output_dir
cd $output_dir

data_path_prefix_numcat=$(basename ${data_path_prefix}_numcat)

python ../../generators/odi-nhs-ae/transform_data_to_numcats.py \
    --input-filename $data_path_prefix \
    --output-filename $data_path_prefix_numcat


python $PREPARE_DATA_PY $parameter_json $data_path_prefix_numcat

echo >log.conf <<END
* GLOBAL:
   FORMAT               =  "[%datetime{%Y-%M-%d %H:%m:%s.%g} <%levshort>] %msg"
   FILENAME             =  "./main.log"
   ENABLED              =  true
   TO_FILE              =  true
   TO_STANDARD_OUTPUT   =  true
   MILLISECONDS_WIDTH   =  3
   PERFORMANCE_TRACKING =  true
   MAX_LOG_FILE_SIZE    =  2097152      ## 64MB - Comment starts with two hashes (##)
   LOG_FLUSH_THRESHOLD  =  100          ## Flush after every 100 logs
* DEBUG:
   FORMAT               = "[%datetime{%d/%M} %func] %msg"       ## "[%datetime{%d/%M} %func] %msg"
* TRACE:
   FORMAT               = "[%datetime] %msg"
   TO_STANDARD_OUTPUT   =  false

END


$SGFINIT my.cfg
$SGFGEN my.cfg

cat ./gen/*.out > ./cand.out
$SGFEXT my.cfg cand.out synthetic_data.csv "$(< privacy_params)"
